{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGT9YuvzdREB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/dataset.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "nvmDheEadtFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "929GzDj0e8tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "oTJZsPmvh15X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "LaaexzZqh3OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = data['Outcome'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# Visualize class distribution\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Outcome', data=data, color='pink')\n",
        "plt.xticks([0, 1], ['Non-Diabetic', 'Diabetic'])\n",
        "plt.title('Result Disribution')\n",
        "plt.xlabel('Result')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xLGXnKh3h6wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(X.columns):\n",
        "  plt.subplot(3, 3, i+1)\n",
        "  sns.boxplot(x='Outcome', y=col, data=data)\n",
        "  plt.title(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SF6PA8VUkkD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Balanced Dataset**"
      ],
      "metadata": {
        "id": "nzlQQZ3zpya7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "sm = SMOTE(random_state=33)\n",
        "X1, y1 = sm.fit_resample(X, y)\n",
        "df = pd.concat([pd.DataFrame(X1), pd.DataFrame(y1, columns=['Outcome'])], axis=1)\n",
        "\n",
        "count = df['Outcome'].value_counts()\n",
        "print(count)"
      ],
      "metadata": {
        "id": "bmB2FEHbiUgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Outcome', data=df, color='pink')\n",
        "plt.xticks([0, 1], ['Non-Diabetic', 'Diabetic'])\n",
        "plt.title('Result Disribution After Balancing')\n",
        "plt.xlabel('Result')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4DzQF6fjjS5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(X.columns):\n",
        "  plt.subplot(3, 3, i+1)\n",
        "  sns.boxplot(x='Outcome', y=col, data=df)\n",
        "  plt.title(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u02XtQahjQU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "yp1FGsvGmZKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df.corr()\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.heatmap(data =corr,annot = True)"
      ],
      "metadata": {
        "id": "ysQ8Y5Gik6hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.drop(columns=['BloodPressure','Insulin','SkinThickness'], axis=1)"
      ],
      "metadata": {
        "id": "tlwPFVu9nsS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "bnBHQPB0oHj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.isna().sum()"
      ],
      "metadata": {
        "id": "L_Q7FHDAo4v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0, 1))\n",
        "df2 = sc.fit_transform(df1)\n",
        "df2 = pd.DataFrame(df2, columns=df1.columns)\n",
        "df2"
      ],
      "metadata": {
        "id": "Pc2BpES_kxD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X1 = df2.drop('Outcome', axis=1)\n",
        "y1 = df2['Outcome']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=33)"
      ],
      "metadata": {
        "id": "rqVvS5xmm5Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "pAUuobDbpQGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_predictions = svm_model.predict(X_test)"
      ],
      "metadata": {
        "id": "hoezuMmapfjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "svm_precision = precision_score(y_test, svm_predictions)\n",
        "svm_recall = recall_score(y_test, svm_predictions)\n",
        "svm_f1 = f1_score(y_test, svm_predictions)\n",
        "svm_conf_matrix = confusion_matrix(y_test, svm_predictions)\n",
        "\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n",
        "print(\"SVM Precision:\", svm_precision)\n",
        "print(\"SVM Recall:\", svm_recall)\n",
        "print(\"SVM F1-Score:\", svm_f1)\n",
        "print(\"SVM Confusion Matrix:\\n\", svm_conf_matrix)"
      ],
      "metadata": {
        "id": "TPE6O1Expil_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(random_state=33)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "HNu0LIvopms-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "rf_precision = precision_score(y_test, rf_predictions)\n",
        "rf_recall = recall_score(y_test, rf_predictions)\n",
        "rf_f1 = f1_score(y_test, rf_predictions)\n",
        "rf_conf_matrix = confusion_matrix(y_test, rf_predictions)\n",
        "\n",
        "print(\"\\nRandom Forest Accuracy:\", rf_accuracy)\n",
        "print(\"Random Forest Precision:\", rf_precision)\n",
        "print(\"Random Forest Recall:\", rf_recall)\n",
        "print(\"Random Forest F1-Score:\", rf_f1)\n",
        "print(\"Random Forest Confusion Matrix:\\n\", rf_conf_matrix)"
      ],
      "metadata": {
        "id": "c_Qt2SVUprpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UnBalanced Dataset"
      ],
      "metadata": {
        "id": "Dn8SPjhXqDYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "XGG0et-EqIFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = data.corr()\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.heatmap(data =corr,annot = True)"
      ],
      "metadata": {
        "id": "jy3w74q1ptBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data.drop(columns=['BloodPressure','SkinThickness'], axis=1)"
      ],
      "metadata": {
        "id": "AgYDx_HZqJ3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data2 = sc.fit_transform(data1)\n",
        "data2 = pd.DataFrame(data2, columns=data1.columns)\n",
        "data2"
      ],
      "metadata": {
        "id": "hql0GjnSqZvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = data2.drop('Outcome', axis=1)\n",
        "y1 = data2['Outcome']\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=33)"
      ],
      "metadata": {
        "id": "F42tWNMgqjRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1_test"
      ],
      "metadata": {
        "id": "tjCDEurYrgVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_ub = SVC(kernel='linear')\n",
        "svm_ub.fit(X1_train, y1_train)\n",
        "svm_ub_pred = svm_ub.predict(X1_test)"
      ],
      "metadata": {
        "id": "aKfYN0NOqyR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_ub_acc = accuracy_score(y1_test, svm_ub_pred)\n",
        "svm_ub_prec = precision_score(y1_test, svm_ub_pred)\n",
        "svm_ub_recall = recall_score(y1_test, svm_ub_pred)\n",
        "svm_ub_f1 = f1_score(y1_test, svm_ub_pred)\n",
        "svm_ub_confMatx = confusion_matrix(y1_test, svm_ub_pred)\n",
        "\n",
        "print('For Unbalanced Dataset')\n",
        "print(\"SVM Accuracy:\", svm_ub_acc)\n",
        "print(\"SVM Precision:\", svm_ub_prec)\n",
        "print(\"SVM Recall:\", svm_ub_recall)\n",
        "print(\"SVM F1-Score:\", svm_ub_f1)\n",
        "print(\"SVM Confusion Matrix:\\n\", svm_ub_confMatx)"
      ],
      "metadata": {
        "id": "20BP4ckrrCK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_ub = RandomForestClassifier(random_state=33)\n",
        "rf_ub.fit(X_train, y_train)\n",
        "rf_ub_pred = rf_ub.predict(X_test)"
      ],
      "metadata": {
        "id": "UWLE-eBUsRWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_ub_acc = accuracy_score(y1_test, rf_ub_pred)\n",
        "rf_ub_prec = precision_score(y1_test, rf_ub_pred)\n",
        "rf_ub_recall = recall_score(y1_test, rf_ub_pred)\n",
        "rf_ub_f1 = f1_score(y1_test, rf_ub_pred)\n",
        "rf_ub_confMatx = confusion_matrix(y1_test, rf_ub_pred)\n",
        "\n",
        "print('For Unbalanced Dataset')\n",
        "print(\"Random Forest Accuracy:\", rf_ub_acc)\n",
        "print(\"Random Forest Precision:\", rf_ub_prec)\n",
        "print(\"Random Forest Recall:\", rf_ub_recall)\n",
        "print(\"Random Forest F1-Score:\", rf_ub_f1)\n",
        "print(\"Random Forest Confusion Matrix:\\n\", rf_ub_confMatx)"
      ],
      "metadata": {
        "id": "qbvUo5ZvsbFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIPJwt4ys1-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison"
      ],
      "metadata": {
        "id": "LzxrFO0EtCgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Model\", \"Dataset\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "\n",
        "table.add_row([\"SVM\", \"Balanced\", svm_accuracy, svm_precision, svm_recall, svm_f1])\n",
        "table.add_row([\"SVM\", \"Unbalanced\", svm_ub_acc, svm_ub_prec, svm_ub_recall, svm_ub_f1])\n",
        "table.add_row([\"Random Forest\", \"Balanced\", rf_accuracy, rf_precision, rf_recall, rf_f1])\n",
        "table.add_row([\"Random Forest\", \"Unbalanced\", rf_ub_acc, rf_ub_prec, rf_ub_recall, rf_ub_f1])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "id": "_78R2CwDtEIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u9pOZxPstZ2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balanced Dataset:\n",
        "In a balanced dataset, the class distribution is adjusted to have roughly equal instances of each class. Imagine a seesaw where both sides carry equal weight. Models trained on balanced data tend to perform better because they learn from a more representative sample of the minority class. Let's break down the observations:\n",
        "\n",
        "1. **Support Vector Machine (SVM)**:\n",
        "   - Accuracy: 0.74\n",
        "   - Precision: 0.78\n",
        "   - Recall: 0.67\n",
        "   - F1-Score: 0.72\n",
        "   - Confusion Matrix:\n",
        "     ```\n",
        "     [[82 19]\n",
        "      [33 66]]\n",
        "     ```\n",
        "   - The SVM model achieves decent overall performance. It correctly identifies both positive and negative instances, but there's room for improvement.\n",
        "\n",
        "2. **Random Forest**:\n",
        "   - Accuracy: 0.8\n",
        "   - Precision: 0.79\n",
        "   - Recall: 0.82\n",
        "   - F1-Score: 0.80\n",
        "   - Confusion Matrix:\n",
        "     ```\n",
        "     [[79 22]\n",
        "      [18 81]]\n",
        "     ```\n",
        "   - The Random Forest model shines here! It achieves higher recall, meaning it captures more true positive cases.\n",
        "\n",
        "## Unbalanced Dataset:\n",
        "For unbalanced dataset, here one class dominates the other.\n",
        "\n",
        "1. **SVM (Unbalanced)**:\n",
        "   - Accuracy: 0.75\n",
        "   - Precision: 0.74\n",
        "   - Recall: 0.45\n",
        "   - F1-Score: 0.56\n",
        "   - Confusion Matrix:\n",
        "     ```\n",
        "     [[90  9]\n",
        "      [30 25]]\n",
        "     ```\n",
        "   - The SVM model struggles. It achieves high precision (few false positives), but recall drops significantly. It misses many positive instances.\n",
        "\n",
        "2. **Random Forest (Unbalanced)**:\n",
        "   - Accuracy: 0.75\n",
        "   - Precision: 0.69\n",
        "   - Recall: 0.56\n",
        "   - F1-Score: 0.62\n",
        "   - Confusion Matrix:\n",
        "     ```\n",
        "     [[85 14]\n",
        "      [24 31]]\n",
        "     ```\n",
        "   - The Random Forest also faces challenges. It maintains decent precision but sacrifices recall.\n",
        "\n",
        "## Insights:\n",
        "- Here **SMOTE** (Synthetic Minority Over-sampling Technique) has improved performance on unbalanced data.\n",
        "- Balanced datasets lead to better generalization, but unbalanced ones make models biased toward the majority class."
      ],
      "metadata": {
        "id": "StWhsK2dvJja"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzxJOqJLvMdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgN4TO9Mwu26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}